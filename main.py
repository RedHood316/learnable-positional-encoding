# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t4YlvxU_rmIoIzz0jiN-w9DlgGlFlEXF
"""

import torch
import torch.nn as nn

# -------------------------------
# Learnable Positional Encoding Class
# -------------------------------
class LearnablePositionalEncoding(nn.Module):
    """
    Implements learnable positional encoding for Transformer models.
    This helps the model understand token positions dynamically instead of using fixed sinusoidal embeddings.
    """
    def __init__(self, max_seq_len, d_model):
        """
        Args:
            max_seq_len (int): Maximum sequence length for positional encoding.
            d_model (int): Embedding dimension for each token.
        """
        super(LearnablePositionalEncoding, self).__init__()

        # Create a learnable positional encoding matrix of shape (max_seq_len, d_model)
        self.positional_encoding = nn.Parameter(torch.empty(max_seq_len, d_model))

        # Initialize using Xavier Uniform to improve training stability
        nn.init.xavier_uniform_(self.positional_encoding)

    def forward(self, x):
        """
        Adds positional encoding to the input token embeddings.

        Args:
            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model)

        Returns:
            Tensor: Input with added positional encoding.
        """
        seq_len = x.shape[1]  # Get the actual sequence length from input
        return x + self.positional_encoding[:seq_len, :]  # Add positional encoding to token embeddings


# -------------------------------
# Simple Transformer Block with Learnable Positional Encoding
# -------------------------------
class SimpleTransformerBlock(nn.Module):
    """
    Implements a basic Transformer block with token embeddings, learnable positional encoding,
    and multi-head self-attention.
    """
    def __init__(self, max_seq_len, d_model, num_heads):
        """
        Args:
            max_seq_len (int): Maximum sequence length for positional encoding.
            d_model (int): Embedding dimension.
            num_heads (int): Number of attention heads.
        """
        super(SimpleTransformerBlock, self).__init__()

        # Token Embedding Layer: Converts token indices to dense vectors
        self.embedding = nn.Embedding(num_embeddings=1000, embedding_dim=d_model)

        # Learnable Positional Encoding Layer
        self.positional_encoding = LearnablePositionalEncoding(max_seq_len, d_model)

        # Multi-Head Self-Attention Layer
        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)

        # Fully Connected Layer (Projection)
        self.fc = nn.Linear(d_model, d_model)

    def forward(self, x):
        """
        Forward pass for the Transformer block.

        Args:
            x (Tensor): Input tokenized sequence of shape (batch_size, seq_len)

        Returns:
            Tensor: Processed sequence embeddings of shape (batch_size, seq_len, d_model)
        """
        x = self.embedding(x)  # Convert token indices to dense embeddings
        x = self.positional_encoding(x)  # Add positional encoding
        x, _ = self.attention(x, x, x)  # Apply self-attention
        return self.fc(x)  # Pass through a fully connected layer


# -------------------------------
# Running the Model with a Dummy Dataset
# -------------------------------
if __name__ == "__main__":
    # Define Hyperparameters
    batch_size = 4
    max_seq_len = 20  # Maximum sequence length for encoding
    seq_len = torch.randint(5, 15, (1,)).item()  # Random sequence length between 5 and 15
    d_model = 32  # Embedding dimension
    num_heads = 4  # Number of self-attention heads

    # Generate a Random Tokenized Input (Simulated Text Sequence)
    dummy_input = torch.randint(0, 1000, (batch_size, seq_len))  # Shape: (batch_size, seq_len)

    # Initialize the Transformer Model
    model = SimpleTransformerBlock(max_seq_len=max_seq_len, d_model=d_model, num_heads=num_heads)

    # Forward Pass
    output = model(dummy_input)

    # Print the Output Shape for Verification
    print("Input Shape:", dummy_input.shape)  # Expected: (batch_size, seq_len)
    print("Output Shape:", output.shape)  # Expected: (batch_size, seq_len, d_model)